{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://numericinsight.com/uploads/A_Gentle_Introduction_to_Backpropagation.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doing Neural Network (MLP)\n",
    "\n",
    "## structure: 1 x 3 x 1\n",
    "### 1 inputs :: 1 hidden layer with 3 units :: 1 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ground_prediction(x1_i, x2_i):\n",
    "    ground = x1_i + x2_i\n",
    "    return ground\n",
    "    \n",
    "    \n",
    "n_samples = 1000\n",
    "X1 = np.random.choice(np.random.randint(0, 10000, 10000), n_samples)\n",
    "X2 = np.random.choice(np.random.randint(10000, 20000, 10000), n_samples)\n",
    "X = np.asarray([X1, X2]).transpose()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(X)\n",
    "X_norm = scaler.transform(X)\n",
    "\n",
    "y = list()\n",
    "for i in np.arange(X.shape[0]):\n",
    "    y_i = ground_prediction(X[i,0], X[i,1])\n",
    "    y.append(y_i)\n",
    "    \n",
    "y = np.asarray(y) \n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y.fit(y)\n",
    "y_norm = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Weight Layers\n",
    "def initialize_layers():\n",
    "    # +1 represents bias purposes\n",
    "    X_feat_n = X.shape[1]\n",
    "    L1_units_n = X_feat_n\n",
    "    L2_units_n = 3\n",
    "    L3_units_n = 1\n",
    "    \n",
    "    L1_weights = np.random.randn(L2_units_n*(L1_units_n + 1))\n",
    "    L1_weights = L1_weights.reshape(L2_units_n, (L1_units_n + 1))\n",
    "\n",
    "    L2_weights = np.random.randn(L3_units_n, (L2_units_n + 1))\n",
    "    L2_weights = L2_weights.reshape(L3_units_n, (L2_units_n + 1))\n",
    "\n",
    "    return L2_weights, L1_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_fun(z):\n",
    "    activation = 1 / (1 + np.exp(-z))\n",
    "    return activation\n",
    " \n",
    "\n",
    "def sigmoid_fun_derivative(a):\n",
    "    derv = np.multiply(a, (1.0 - a))\n",
    "    return derv\n",
    "    \n",
    "    \n",
    "def add_bias(a_matrix):\n",
    "    a_matrix = np.row_stack((np.ones(a_matrix.shape[1]), a_matrix))\n",
    "    return a_matrix\n",
    "\n",
    "\n",
    "def compute_mean_squared_error(a_3, idx):\n",
    "    y_3 = y_norm[idx].reshape(a_3.shape)\n",
    "    err = 0.5*np.power(y_3 - a_3, 2)\n",
    "    \n",
    "    err = np.sum(err)/batch_size\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_backpropagation_errors(a_3, a_2):    \n",
    "    #d_3 = np.multiply((a_3 - y[idx].reshape(a_3.shape)), sigmoid_fun_derivative(a_3))\n",
    "    d_3 = a_3 - y_norm[idx].reshape(a_3.shape)\n",
    "    \n",
    "    # remove the bias column of weights and its activation\n",
    "    d_2 = np.multiply(L2_weights[:, 1:].transpose().dot(d_3), sigmoid_fun_derivative(a_2[1:]))\n",
    "    \n",
    "    return d_3, d_2\n",
    "\n",
    "\n",
    "def compute_gradient(d, a):\n",
    "    grad = d.dot(a.transpose()) / batch_size\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def update_weights(a_3, a_2, a_1, check_grad=False):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    d_3, d_2 = compute_backpropagation_errors(a_3, a_2)\n",
    "    \n",
    "    grad_L2 = compute_gradient(d_3, a_2)\n",
    "    grad_L1 = compute_gradient(d_2, a_1)\n",
    "    \n",
    "    L2_weights_updated = L2_weights - r * grad_L2\n",
    "    L1_weights_updated = L1_weights - r * grad_L1\n",
    "    \n",
    "    if (check_grad):\n",
    "        check_gradient(L2_weights, grad_L2, L1_weights, grad_L1)\n",
    "    \n",
    "    return L2_weights_updated, L1_weights_updated\n",
    "\n",
    "\n",
    "def feed_forward_NN(L2_weights, L1_weights, idx):\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    a_1 = X_norm[idx].transpose()\n",
    "    a_1 = add_bias(a_1)\n",
    "    \n",
    "    z_2 = L1_weights.dot(a_1)\n",
    "    a_2 = sigmoid_fun(z_2)\n",
    "    a_2 = add_bias(a_2)\n",
    "    \n",
    "    z_3 = L2_weights.dot(a_2)\n",
    "    #a_3 = sigmoid_fun(z_3)\n",
    "    a_3 = z_3\n",
    "    \n",
    "    return a_3, a_2, a_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient_approximation(L2_weights, L1_weights, e):\n",
    "    # 2nd layer\n",
    "    L2_weights_gradsAprox = np.zeros(L2_weights.shape)\n",
    "    for i in np.arange(L2_weights.shape[0]):\n",
    "        for j in np.arange(L2_weights.shape[1]):\n",
    "            L2_weights_shift = np.copy(L2_weights)\n",
    "            \n",
    "            L2_weights_shift[i,j] = L2_weights[i,j] + e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights_shift, L1_weights, idx)\n",
    "            err_plus = compute_mean_squared_error(a_3, idx)\n",
    "\n",
    "            L2_weights_shift[i,j] = L2_weights[i,j] - e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights_shift, L1_weights, idx)\n",
    "            err_minus = compute_mean_squared_error(a_3, idx)\n",
    "    \n",
    "            derv = (err_plus - err_minus)/(2*e)\n",
    "            L2_weights_gradsAprox[i,j] = derv\n",
    "            \n",
    "           \n",
    "    # 1st layer\n",
    "    L1_weights_gradsAprox = np.zeros(L1_weights.shape)\n",
    "    for i in np.arange(L1_weights.shape[0]):\n",
    "        for j in np.arange(L1_weights.shape[1]):\n",
    "            L1_weights_shift = np.copy(L1_weights)\n",
    "            \n",
    "            L1_weights_shift[i,j] = L1_weights[i,j] + e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights_shift, idx)\n",
    "            err_plus = compute_mean_squared_error(a_3, idx)\n",
    "\n",
    "            L1_weights_shift[i,j] = L1_weights[i,j] - e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights_shift, idx)\n",
    "            err_minus = compute_mean_squared_error(a_3, idx)\n",
    "    \n",
    "            derv = (err_plus - err_minus)/(2*e)\n",
    "            L1_weights_gradsAprox[i,j] = derv\n",
    "            \n",
    "            \n",
    "    return L2_weights_gradsAprox, L1_weights_gradsAprox\n",
    "    \n",
    "\n",
    "\n",
    "def check_gradient(L2_weights, grad_L2, L1_weights, grad_L1, e=1e-4):\n",
    "        \n",
    "    L2_weights_gradsAprox, L1_weights_gradsAprox = compute_gradient_approximation(L2_weights, L1_weights, e)\n",
    "    \n",
    "    L2_grads_check = np.allclose(L2_weights_gradsAprox, grad_L2, atol=1e-3)\n",
    "    if not (L2_grads_check): sys.exit(\"Error in L2 gradients values checking \")\n",
    "    \n",
    "    L2_grads_check_shape = (L2_weights_gradsAprox.shape == grad_L2.shape)\n",
    "    if not (L2_grads_check_shape): sys.exit(\"Error in L2 gradients shape checking \")\n",
    "        \n",
    "    L1_grads_check = np.allclose(L1_weights_gradsAprox, grad_L1, atol=1e-2)    \n",
    "    if not (L1_grads_check): sys.exit(\"Error in L1 gradients values checking\")\n",
    "\n",
    "    L1_grads_check_shape = (L1_weights_gradsAprox.shape == grad_L1.shape)\n",
    "    if not (L1_grads_check_shape): sys.exit(\"Error in L1 gradients shape checking \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2_weights\n",
      "[[ 0.15505334  0.7117675   0.10145171  1.29070653]]\n",
      "L1_weights\n",
      "[[ 1.51178037  1.16633675  0.01330648]\n",
      " [ 1.37712699  0.61443771  0.95080865]\n",
      " [-0.20190246 -1.22338544  1.25536653]]\n",
      "\n",
      "[[-1.30052695  1.32049565  0.13984626  1.08100489]]\n",
      "[[ 0.11151099  1.59280136  0.31995821]\n",
      " [ 1.44856551  0.49401796  0.91232369]\n",
      " [-0.17145282  0.15631962  1.70884436]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "err_history = []\n",
    "\n",
    "itr_n = 1e4\n",
    "batch_size = 500\n",
    "r = 90e-3\n",
    "        \n",
    "L2_weights, L1_weights = initialize_layers()\n",
    "print \"L2_weights\"\n",
    "print L2_weights\n",
    "print \"L1_weights\"\n",
    "print L1_weights\n",
    "\n",
    "CG = True\n",
    "for i in np.arange(itr_n):\n",
    "    idx = np.random.choice(np.arange(X.shape[0]), batch_size)\n",
    "    a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights, idx)\n",
    "\n",
    "    err = compute_mean_squared_error(a_3, idx)\n",
    "\n",
    "    #print err\n",
    "    err_history.append(err)\n",
    "    if (i == 5): CG = False;        \n",
    "    L2_weights, L1_weights = update_weights(a_3, a_2, a_1, check_grad=CG)\n",
    "\n",
    "\n",
    "print \"\"\n",
    "print L2_weights\n",
    "print L1_weights   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQaAJDXKAJCkRv0/x4F7ebOgns4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b07523c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(err_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22903 23135 -232\n",
      "19770 19950 -180\n",
      "22277 22548 -271\n",
      "23513 23813 -300\n",
      "18123 17901 222\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(np.arange(X.shape[0]), 5)\n",
    "a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights, idx)\n",
    "\n",
    "y_demo = y[idx]\n",
    "y_demo = y_demo.reshape((5,))\n",
    "a_3_demo = scaler_y.inverse_transform(a_3) \n",
    "a_3_demo = a_3_demo.reshape((5,))\n",
    "\n",
    "for i in np.arange(y_demo.shape[0]):\n",
    "    print int(y_demo[i]), int(a_3_demo[i]), (int(y_demo[i]) - int(a_3_demo[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41731503 -0.43606    -0.04902948  0.62667192 -0.2819407   0.47211696\n",
      "   0.19478532 -0.42585398 -0.26800462  0.57903749]]\n",
      "[[ 24013.50682788  15723.39511686  19483.20313198  26047.30431953\n",
      "   17220.58711498  24545.8802296   21851.74194966  15822.54147139\n",
      "   17355.96911055  25584.55972926]]\n"
     ]
    }
   ],
   "source": [
    "a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights, np.random.choice(np.arange(X.shape[0]), 10))\n",
    "print a_3\n",
    "print scaler_y.inverse_transform(a_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
