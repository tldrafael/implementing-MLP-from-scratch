{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://numericinsight.com/uploads/A_Gentle_Introduction_to_Backpropagation.pdf\n",
    "#\n",
    "# creating a vanilla Neural Network (MLP) from scratch\n",
    "#\n",
    "# structure: 1 x 3 x 1\n",
    "# 1 inputs ---- 1 hidden layer with 3 units ---- 1 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ground_prediction(x1_i, x2_i):\n",
    "    ground = np.bitwise_or(x1_i, x2_i)\n",
    "    return ground\n",
    "    \n",
    "    \n",
    "n_samples = 1000\n",
    "X1 = np.random.choice(np.random.randint(0, 10000, 10000), n_samples)\n",
    "X2 = np.random.choice(np.random.randint(10000, 20000, 10000), n_samples)\n",
    "X1 = np.random.choice(np.random.randint(0, 2, 10000), n_samples)\n",
    "X2 = np.random.choice(np.random.randint(0, 2, 10000), n_samples)\n",
    "X = np.asarray([X1, X2]).transpose()\n",
    "X_norm = X\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler.fit(X)\n",
    "# X_norm = scaler.transform(X)\n",
    "\n",
    "y = list()\n",
    "for i in np.arange(X.shape[0]):\n",
    "    y_i = ground_prediction(X[i,0], X[i,1])\n",
    "    y.append(y_i)\n",
    "    \n",
    "y = np.asarray(y) \n",
    "y_norm = y\n",
    "\n",
    "# y = y.reshape(-1,1)\n",
    "# scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler_y.fit(y)\n",
    "# y_norm = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]]\n",
      "[1 0 0 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print X_norm[1:15]\n",
    "print y_norm[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Weight Layers\n",
    "def initialize_layers():\n",
    "    # +1 represents bias purposes\n",
    "    X_feat_n = X.shape[1]\n",
    "    L1_units_n = X_feat_n\n",
    "    L2_units_n = 3\n",
    "    L3_units_n = 1\n",
    "    \n",
    "    L1_weights = np.random.randn(L2_units_n*(L1_units_n + 1))\n",
    "    L1_weights = L1_weights.reshape(L2_units_n, (L1_units_n + 1))\n",
    "\n",
    "    L2_weights = np.random.randn(L3_units_n, (L2_units_n + 1))\n",
    "    L2_weights = L2_weights.reshape(L3_units_n, (L2_units_n + 1))\n",
    "\n",
    "    return L2_weights, L1_weights\n",
    "\n",
    "\n",
    "def sigmoid_fun(z):\n",
    "    activation = 1 / (1 + np.exp(-z))\n",
    "    return activation\n",
    " \n",
    "\n",
    "def sigmoid_fun_derivative(a):\n",
    "    derv = np.multiply(a, (1.0 - a))\n",
    "    return derv\n",
    "    \n",
    "    \n",
    "def add_bias(a_matrix):\n",
    "    a_matrix = np.row_stack((np.ones(a_matrix.shape[1]), a_matrix))\n",
    "    return a_matrix\n",
    "\n",
    "\n",
    "def compute_mean_squared_error(a_3, idx):\n",
    "    y_3 = y_norm[idx].reshape(a_3.shape)\n",
    "    err = 0.5*np.power(y_3 - a_3, 2)\n",
    "    \n",
    "    err = np.sum(err)/batch_size\n",
    "    return err\n",
    "\n",
    "\n",
    "def log_likelihood(a_3, idx):\n",
    "    y_3 = y_norm[idx].reshape(a_3.shape)\n",
    "    \n",
    "    ll = np.multiply(y_3, np.log(a_3)) + np.multiply(1 - y_3, np.log(1 - a_3))\n",
    "    ll = np.sum(ll)/batch_size\n",
    "    \n",
    "    return ll\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_backpropagation_errors(L2_weights, a_3, a_2, idx):    \n",
    "    #d_3 = a_3 - y_norm[idx].reshape(a_3.shape)\n",
    "    d_3 = np.multiply((a_3 - y_norm[idx].reshape(a_3.shape)), sigmoid_fun_derivative(a_3))\n",
    "    \n",
    "    # remove the bias column of weights and its activation\n",
    "    d_2 = np.multiply(L2_weights[:, 1:].transpose().dot(d_3), sigmoid_fun_derivative(a_2[1:]))\n",
    "    \n",
    "    return d_3, d_2\n",
    "\n",
    "\n",
    "def compute_gradient(d, a):\n",
    "    grad = d.dot(a.transpose()) / batch_size\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def update_weights(L2_weights, L1_weights, a_3, a_2, a_1, idx, check_grad=False):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    d_3, d_2 = compute_backpropagation_errors(L2_weights, a_3, a_2, idx)\n",
    "    \n",
    "    grad_L2 = compute_gradient(d_3, a_2)\n",
    "    grad_L1 = compute_gradient(d_2, a_1)\n",
    "    \n",
    "    L2_weights_updated = L2_weights - r * grad_L2\n",
    "    L1_weights_updated = L1_weights - r * grad_L1\n",
    "    \n",
    "    if (check_grad):\n",
    "        check_gradient(L2_weights, grad_L2, L1_weights, grad_L1, idx)\n",
    "    \n",
    "    return L2_weights_updated, L1_weights_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_gradient_approximation(L2_weights, L1_weights, idx, e):\n",
    "    # 2nd layer\n",
    "    L2_weights_gradsAprox = np.zeros(L2_weights.shape)\n",
    "    for i in np.arange(L2_weights.shape[0]):\n",
    "        for j in np.arange(L2_weights.shape[1]):\n",
    "            L2_weights_shift = np.copy(L2_weights)\n",
    "            \n",
    "            L2_weights_shift[i,j] = L2_weights[i,j] + e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights_shift, L1_weights, idx)\n",
    "            err_plus = compute_mean_squared_error(a_3, idx)\n",
    "\n",
    "            L2_weights_shift[i,j] = L2_weights[i,j] - e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights_shift, L1_weights, idx)\n",
    "            err_minus = compute_mean_squared_error(a_3, idx)\n",
    "    \n",
    "            derv = (err_plus - err_minus)/(2*e)\n",
    "            L2_weights_gradsAprox[i,j] = derv\n",
    "            \n",
    "           \n",
    "    # 1st layer\n",
    "    L1_weights_gradsAprox = np.zeros(L1_weights.shape)\n",
    "    for i in np.arange(L1_weights.shape[0]):\n",
    "        for j in np.arange(L1_weights.shape[1]):\n",
    "            L1_weights_shift = np.copy(L1_weights)\n",
    "            \n",
    "            L1_weights_shift[i,j] = L1_weights[i,j] + e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights_shift, idx)\n",
    "            err_plus = compute_mean_squared_error(a_3, idx)\n",
    "\n",
    "            L1_weights_shift[i,j] = L1_weights[i,j] - e\n",
    "            a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights_shift, idx)\n",
    "            err_minus = compute_mean_squared_error(a_3, idx)\n",
    "    \n",
    "            derv = (err_plus - err_minus)/(2*e)\n",
    "            L1_weights_gradsAprox[i,j] = derv\n",
    "            \n",
    "            \n",
    "    return L2_weights_gradsAprox, L1_weights_gradsAprox\n",
    "    \n",
    "\n",
    "\n",
    "def check_gradient(L2_weights, grad_L2, L1_weights, grad_L1, idx, e=1e-4):\n",
    "        \n",
    "    L2_weights_gradsAprox, L1_weights_gradsAprox = compute_gradient_approximation(L2_weights, L1_weights, idx, e)\n",
    "    \n",
    "    L2_grads_check = np.allclose(L2_weights_gradsAprox, grad_L2, atol=1e-3)\n",
    "    if not (L2_grads_check): sys.exit(\"Error in L2 gradients values checking \")\n",
    "    \n",
    "    L2_grads_check_shape = (L2_weights_gradsAprox.shape == grad_L2.shape)\n",
    "    if not (L2_grads_check_shape): sys.exit(\"Error in L2 gradients shape checking \")\n",
    "        \n",
    "    L1_grads_check = np.allclose(L1_weights_gradsAprox, grad_L1, atol=1e-2)    \n",
    "    if not (L1_grads_check): sys.exit(\"Error in L1 gradients values checking\")\n",
    "\n",
    "    L1_grads_check_shape = (L1_weights_gradsAprox.shape == grad_L1.shape)\n",
    "    if not (L1_grads_check_shape): sys.exit(\"Error in L1 gradients shape checking \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.L2_weights, self.L1_weights = initialize_layers()\n",
    "        self.err_history = []\n",
    "        self.ll_history = []\n",
    "    \n",
    "        # Activation Layer's States\n",
    "        self.a_3 = np.array([])\n",
    "        self.a_2 = np.array([])\n",
    "        self.a_1 = np.array([])\n",
    "    \n",
    "    \n",
    "    def feed_forward_NN(self, idx, predict=False):\n",
    "        self.a_1 = X_norm[idx, :].transpose()\n",
    "        self.a_1 = add_bias(self.a_1)\n",
    "\n",
    "        z_2 = self.L1_weights.dot(self.a_1)\n",
    "        self.a_2 = sigmoid_fun(z_2)\n",
    "        self.a_2 = add_bias(self.a_2)\n",
    "\n",
    "        z_3 = self.L2_weights.dot(self.a_2)\n",
    "        #a_3 = z_3\n",
    "        self.a_3 = sigmoid_fun(z_3)\n",
    "\n",
    "        \n",
    "        if predict: \n",
    "            return self.a_3        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def update_weights(self, idx, check_grad=False):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        d_3, d_2 = compute_backpropagation_errors(self.L2_weights, self.a_3, self.a_2, idx)\n",
    "\n",
    "        grad_L2 = compute_gradient(d_3, self.a_2)\n",
    "        grad_L1 = compute_gradient(d_2, self.a_1)\n",
    "\n",
    "        self.L2_weights +=  (- r * grad_L2)\n",
    "        self.L1_weights +=  (- r * grad_L1)\n",
    "\n",
    "        if (check_grad):\n",
    "            check_gradient(self.L2_weights, grad_L2, self.L1_weights, grad_L1, idx)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def train(self, itr_n=100, batch_size=32, r=0.3, CG = True):\n",
    "\n",
    "        for i in np.arange(itr_n):\n",
    "            idx = np.random.choice(np.arange(X.shape[0]), batch_size)\n",
    "            self.feed_forward_NN(idx)\n",
    "\n",
    "            err = compute_mean_squared_error(self.a_3, idx)\n",
    "            self.err_history.append(err)\n",
    "\n",
    "            ll = log_likelihood(self.a_3, idx)\n",
    "            self.ll_history.append(ll)\n",
    "\n",
    "            if (i == 5): CG = False;        \n",
    "            self.update_weights(idx, check_grad=CG)\n",
    "    \n",
    "    \n",
    "    def predict(self, idx):\n",
    "        return self.feed_forward_NN(idx, predict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itr_n = 10\n",
    "batch_size = 1000\n",
    "r = 0.3\n",
    "\n",
    "nn_test = NeuralNet()\n",
    "nn_test.train(itr_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38014235]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test.predict([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(ll_history)\n",
    "plt.plot(err_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#idx = np.random.choice(np.arange(X.shape[0]), 5)\n",
    "idx = np.arange(y.shape[0])\n",
    "a_3, a_2, a_1 = feed_forward_NN(L2_weights, L1_weights, idx)\n",
    "\n",
    "y_demo = y[idx]\n",
    "#y_demo = y_demo.reshape((idx.shape[0],))\n",
    "#a_3_demo = scaler_y.inverse_transform(a_3) \n",
    "a_3_demo = a_3\n",
    "a_3_demo = a_3_demo.reshape((idx.shape[0],))\n",
    "y_diff = y_demo - a_3_demo\n",
    "y_diff_round = y_demo - np.round(a_3_demo)\n",
    "\n",
    "# for i in np.arange(y_demo.shape[0]):\n",
    "#     print int(y_demo[i]), int(a_3_demo[i]), int(y_diff[i])\n",
    "    \n",
    "for perc in np.arange(0, 100+10, 10):\n",
    "    print \"perc: %s  /  %s\" %(perc, np.percentile(y_diff, perc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.percentile(y_diff_round, np.arange(0,110,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sig = np.arange(-7.5,7.5, step=.1)\n",
    "y_sig = sigmoid_fun(X_sig)\n",
    "y_sig_der = np.multiply(y_sig, 1 - y_sig)\n",
    "y_sig_der_2 = np.multiply(np.multiply(y_sig, 1 - y_sig), 1 - 2*y_sig)\n",
    "#y_sig_der = sigmoid_fun_derivative(y_sig)\n",
    "\n",
    "plt.plot(X_sig, y_sig)\n",
    "plt.plot(X_sig, y_sig_der)\n",
    "plt.plot(X_sig, y_sig_der_2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
