This small project is for my education purpose, which i try to implement in Python the main concepts of a vanilla **Multilayer Perceptron**.
  
The concepts been worked here are:
  - Feedfoward process
  - Backpropagation
  - Gradient Checking
  - Create distinct neural net structures
  - Use distinct batch sizes

  
The scripts allows:
  - To work with two problems: `linear-regression` and `logistic-regression`;
  - To instance a neural net with any number of hidden layers as its number of units;
  - To instance a hidden layer with n-dimensional as needed;
  - To use or not bias as wished;
  - To use gradient checking;
  - To set any size of batch;

  
The restrictions are:
  - Only work with an output of one dimension;
  - All activation functions are `softmax function`;


***

To see an example of how train a neural net example, check the [main](main.ipynb) notebook.

  

